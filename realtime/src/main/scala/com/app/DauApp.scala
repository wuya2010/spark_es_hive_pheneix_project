package com.app

import java.text.SimpleDateFormat
import java.util.Date

import com.alibaba.fastjson.JSON
import com.util.common.{KafkaUtil, RedisUtil}
import kafka.Kafka
import org.apache.spark.SparkConf
import org.apache.spark.streaming.dstream.InputDStream
import org.apache.spark.streaming.{Seconds, StreamingContext}
import redis.clients.jedis.Jedis

import scala.bean.StartupLog

/**
  * @author kylinWang
  * @data 2020/3/26 16:29
  *
  */
object DauApp {
  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setAppName("DauApp").setMaster("local[2]")
    val ssc = new StreamingContext(conf, Seconds(3)) //fixme: 这里的时间是什么？
    val sourceDStream: InputDStream[(String, String)] = KafkaUtil.getKafkaStream(ssc, "start_topic")

    // 1. 封装数据 , 返回的是 log
    val startupLogDStream = sourceDStream.map {
      case (_, value) =>
        val log = JSON.parseObject(value, classOf[StartupLog])
        log.logDate = new SimpleDateFormat("yyyy-MM-dd").format(log.ts)
        log.logHour = new SimpleDateFormat("HH").format(log.ts)
        log
    }

    //redis 进行去重
    // transform 与 map 的区别
    // transform: Return a new DStream in which each RDD is generated by applying a function on each RDD of 'this' DStream.

    var filteredDStream = startupLogDStream.transform(rdd => {
      //fixme: 建立 redis 的连接
      val client = RedisUtil.getJedisClient

      //Return all the members (elements) of the set value stored at key
      // fixme: smebers: 返回名称为key的set的所有元素
      val uidSet = client.smembers("startup0508" + new SimpleDateFormat("yyyy-MM-dd").format(new Date()))

      //将变量广播出去
      val uidSetBC = ssc.sparkContext.broadcast(uidSet) //fixme: 2种广播变量的方式
      //过滤已经启动过的
      rdd.filter(log => {
        !uidSetBC.value.contains(log.uid)
      } //只保留在 redis 中的数据
      )
    })


    //fixme: 考虑到某个用户第一次启动所在的批次中出现该用户多次启动
    filteredDStream = filteredDStream.map(log => (log.uid, log))
      .groupByKey()
      .map {
        case (_, logIt) => logIt.toList.sortBy(_.ts).head //根据时间排序，取第一条
      }


    // 2.2 写入到redis中 只写mid  (表示已经启动过的设备)
    filteredDStream.foreachRDD(rdd => {
      rdd.foreachPartition(logIt => {
        val client: Jedis = RedisUtil.getJedisClient
        logIt.foreach(log => {
          client.sadd("topic_start" + ":" + log.logDate, log.uid) //写入redis
        })
        client.close()
      })
    })


    import org.apache.phoenix.spark._
    // 3. 写到 hbase
    filteredDStream.foreachRDD(rdd => {
      //fixme: 公用方法, saveTo （ tableName, cols,conf, zkUrl ）
      rdd.saveToPhoenix(
        "GMALL0508_DAU",
        Seq("MID", "UID", "APPID", "AREA", "OS", "CH", "TYPE", "VS", "TS", "LOGDATE", "LOGHOUR"),
        zkUrl = Some("hadoop201,hadoop202,hadoop203:2181")
      )
    })

    ssc.start()
    ssc.awaitTermination()


  }
}
